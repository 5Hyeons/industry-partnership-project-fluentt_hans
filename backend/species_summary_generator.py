# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import os
import re
import json
from dotenv import load_dotenv
from openai import OpenAI

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(os.path.join(os.path.abspath(".."), ".env"))
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# âœ… ê²½ë¡œ ì„¤ì •
BASE_PATH = os.path.dirname(os.path.abspath(__file__))  # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •
DATA_PATH = os.path.join(BASE_PATH, "data", "species")
FINAL_PATH = os.path.join(DATA_PATH, "final_species.json")
FANDOM_PATH = os.path.join(DATA_PATH, "fandom_species.json")
NAMU_PATH = os.path.join(DATA_PATH, "namuwiki_species.json")
WIKI_PATH = os.path.join(DATA_PATH, "wiki_species.json")

# âœ… í¬ë¡¤ëŸ¬ í•¨ìˆ˜
from crawlers.wiki_crawler import get_wiki_species
from crawlers.fandom_crawler import crawl_fandom

# âœ… JSON ì…ì¶œë ¥ í•¨ìˆ˜
def load_json(path, default=[]):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return default

def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# âœ… GPT ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ
def extract_json_block(text):
    match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    if match:
        return match.group(1)
    return text.strip()

# âœ… ê° ì¶œì²˜ ìš”ì•½
def summarize_each_source(sources):
    if not sources:
        print("âš ï¸ ìš”ì•½í•  ì¶œì²˜ê°€ ì—†ìŠµë‹ˆë‹¤ (ë¹ˆ ë¦¬ìŠ¤íŠ¸)")
        return []

    summaries = []
    for i, src in enumerate(sources):
        if not src.strip():
            print(f"âš ï¸ ì¶œì²˜ {i+1}ê°€ ë¹„ì–´ ìˆìŒ â†’ ê±´ë„ˆëœ€")
            continue

        try:
            print(f"ğŸ“„ ì¶œì²˜ {i+1} ìš”ì•½ ì¤‘...")
            res = client.chat.completions.create(
                model="gpt-4o",
                temperature=0.5,
                messages=[
                    {"role": "system", "content": "ì•„ë˜ ê¸€ì„ í•œêµ­ì–´ë¡œ 500ì ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜."},
                    {"role": "user", "content": src}
                ]
            )
            summary = res.choices[0].message.content.strip()
            summaries.append(summary)
        except Exception as e:
            print(f"âŒ ì¶œì²˜ {i+1} ìš”ì•½ ì‹¤íŒ¨: {e}")
    return summaries

# âœ… ìš”ì•½ í•©ì¹˜ê¸°
def reduce_summaries_to_final_json(species_name, summaries):
    if not summaries:
        print("âŒ ìš”ì•½í•  ë‚´ìš© ì—†ìŒ")
        return None

    joined = "\n".join(summaries)
    final_prompt = (
        f"ë‹¤ìŒì€ '{species_name}' ì¢…ì¡±ì— ëŒ€í•œ ì—¬ëŸ¬ ì¶œì²˜ ìš”ì•½ì…ë‹ˆë‹¤. "
        f"ì´ë¥¼ ì¢…í•©í•˜ì—¬ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{joined}\n\n"
        f"{{\n"
        f"    \"ì¢…ì¡±ëª…\": \"{species_name}\",\n"
        f"    \"ìš”ì•½\": \"<5000ì ì´ë‚´ ìš”ì•½ë¬¸>\",\n"
        f"    \"íŠ¹ì§•\": [\"<íŠ¹ì§•1>\", \"<íŠ¹ì§•2>\", \"...\"],\n"
        f"    \"ë§íˆ¬\": {{\n"
        f"        \"ìŠ¤íƒ€ì¼\": \"<ë§íˆ¬ ìŠ¤íƒ€ì¼ ì„¤ëª…>\",\n"
        f"        \"ì¢…ê²°ì–´ë¯¸\": \"<ì˜ˆ: ~ëƒ¥, ~ë‹¤ì˜¹ ë“±>\",\n"
        f"        \"ì˜ˆì‹œ\": \"<ë§íˆ¬ ì˜ˆì‹œ ë¬¸ì¥>\"\n"
        f"    }}\n"
        f"}}"
    )

    try:
        final_res = client.chat.completions.create(
            model="gpt-4o",
            temperature=0.6,
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì¹œì ˆí•˜ê³  ì²´ê³„ì ì¸ ë°±ê³¼ì‚¬ì „ í¸ì§‘ìì•¼. ë°˜ë“œì‹œ JSON í˜•ì‹ì„ ì§€ì¼œ."},
                {"role": "user", "content": final_prompt}
            ]
        )
        content = final_res.choices[0].message.content.strip()
        json_text = extract_json_block(content)
        return json.loads(json_text)
    except Exception as e:
        print(f"âŒ ìµœì¢… ìš”ì•½ ì‹¤íŒ¨: {e}")
        return None

# âœ… ì¢…ì¡± ìš”ì•½ ì‹¤í–‰
def get_or_generate_species_summary(species_name):
    final_data = load_json(FINAL_PATH)

    for entry in final_data:
        if entry["ì¢…ì¡±"] == species_name:
            print(f"âœ… ì¢…ì¡± '{species_name}' ìš”ì•½ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            return entry["ìš”ì•½"]

    print(f"ğŸ” ì¢…ì¡± '{species_name}' ìš”ì•½ì´ ì—†ìŒ â†’ í¬ë¡¤ë§ ë° ë°ì´í„° ì¡°íšŒ ì‹œì‘")

    sources = []

    fandom = crawl_fandom(species_name)
    if fandom:
        print(f"â­ íŒ¬ë¤ í¬ë¡¤ë§ ì„±ê³µ")
        sources.append(fandom)
    else:
        print(f"âŒ íŒ¬ë¤ í¬ë¡¤ë§ ì‹¤íŒ¨")

    wiki = get_wiki_species(species_name)
    if wiki:
        print(f"ğŸ“˜ ìœ„í‚¤ í¬ë¡¤ë§ ì„±ê³µ")
        sources.append(wiki)
    else:
        print(f"âŒ ìœ„í‚¤ í¬ë¡¤ë§ ì‹¤íŒ¨")

    namu_dict = load_json(NAMU_PATH, {})
    if species_name in namu_dict:
        print(f"ğŸŒ¿ ë‚˜ë¬´ìœ„í‚¤ ì €ì¥ë³¸ ìˆ˜ì§‘ ì™„ë£Œ")
        sources.append(namu_dict[species_name])
    else:
        print(f"âŒ ë‚˜ë¬´ìœ„í‚¤ ì €ì¥ë³¸ ì—†ìŒ")

    if sources:
        print(f"ğŸ§  GPT ìš”ì•½ ì‹œì‘ (ì¶œì²˜ ìˆ˜: {len(sources)})")
        partials = summarize_each_source(sources)

        if partials:
            summary = reduce_summaries_to_final_json(species_name, partials)
        else:
            print("âŒ ë¶€ë¶„ ìš”ì•½ ëª¨ë‘ ì‹¤íŒ¨ ë˜ëŠ” ì¶œì²˜ ì—†ìŒ")
            return None
    else:
        print("ğŸ“­ ì¶œì²˜ ì—†ìŒ â†’ ì´ë¦„ë§Œìœ¼ë¡œ ìš”ì•½ ì‹œë„")
        summary = reduce_summaries_to_final_json(species_name, [f"'{species_name}'ì´ë¼ëŠ” ì´ë¦„ë§Œìœ¼ë¡œ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”."])

    if not summary:
        print(f"âŒ GPT ìš”ì•½ ì‹¤íŒ¨: {species_name}")
        return None

    final_data.append({"ì¢…ì¡±": species_name, "ìš”ì•½": summary})
    save_json(FINAL_PATH, final_data)
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {species_name}")
    return summary
