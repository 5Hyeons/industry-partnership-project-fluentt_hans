import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
from deep_translator import GoogleTranslator

# âœ… í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ ê²½ë¡œ
base_path = os.path.dirname(os.path.abspath(__file__))  # â† ì´ ì¤„ ì¶”ê°€
BASE_DIR = os.path.abspath(os.path.join(base_path, ".."))
SAVE_PATH = os.path.join(BASE_DIR, "data", "species", "fandom_species.json")

# âœ… JSON ë¡œë“œ ë° ì €ì¥ í•¨ìˆ˜
def load_json(path, default):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return default

def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# âœ… ì´ë¯¸ ìˆ˜ì§‘í•œ ì¢…ì¡± í™•ì¸
def exists_species(data, species_name):
    return any(entry["ì¢…ì¡±"] == species_name for entry in data)

# âœ… í•œê¸€ â†’ ì˜ì–´ ë²ˆì—­ê¸° (ë„ì–´ì“°ê¸° â†’ ì–¸ë”ìŠ¤ì½”ì–´)
def translate_ko_to_en(text: str) -> str:
    try:
        translated = GoogleTranslator(source='ko', target='en').translate(text)
        return translated.replace(" ", "_")
    except Exception as e:
        print(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {text} â†’ {e}")
        return text.replace(" ", "_")

# âœ… Fandom í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_fandom(species_name_ko):
    species_name_en = translate_ko_to_en(species_name_ko)
    print(f"ğŸŒ ë²ˆì—­ë¨: '{species_name_ko}' â†’ '{species_name_en}'")

    url = f"https://fiction.fandom.com/wiki/{quote_plus(species_name_en)}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        res = requests.get(url, headers=headers, timeout=10)
        if res.status_code != 200:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {species_name_en} ({res.status_code})")
            return None

        soup = BeautifulSoup(res.text, "html.parser")
        content_div = soup.find("div", class_="mw-parser-output")
        if not content_div:
            print(f"âŒ ë³¸ë¬¸ ì—†ìŒ: {species_name_en}")
            return None

        paragraphs = content_div.find_all("p")
        text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        if len(text) < 300:
            print(f"âš ï¸ ë„ˆë¬´ ì§§ìŒ (ê±´ë„ˆëœ€): {species_name_en}")
            return None

        return text

    except Exception as e:
        print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {species_name_en} â†’ {e}")
        return None

# âœ… ë‹¨ì¼ ì¢…ì¡± ì²˜ë¦¬ í•¨ìˆ˜
def process_species(species_name_ko):
    data = load_json(SAVE_PATH, [])

    if exists_species(data, species_name_ko):
        print(f"âš ï¸ ì´ë¯¸ ìˆ˜ì§‘ë¨: {species_name_ko}")
        return

    content = crawl_fandom(species_name_ko)
    if content:
        data.append({"ì¢…ì¡±": species_name_ko, "íŒ¬ë¤_ë‚´ìš©": content})
        save_json(SAVE_PATH, data)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {species_name_ko}")
    else:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {species_name_ko}")